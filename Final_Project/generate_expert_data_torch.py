import os
import numpy as np
import gymnasium as gym
import torch
from agents.simple_ppo_torch import SimplePPO
from agents.double_dqn_torch import DoubleDQNAgent 

# === âš™ï¸ é…ç½®åŒºåŸŸ (æ ¹æ®ä½ çš„æˆªå›¾ä¿®æ”¹) ===
ENV_NAME = "CartPole-v1"
OUTPUT_DIR = "data"
OUTPUT_FILENAME = "expert_data_n5000.npz"
NUM_SAMPLES = 5000  # ç”Ÿæˆå¤šå°‘æ¡æ•°æ®


ALGO = "ppo"
MODEL_PATH = "Final_Project/models/best_ppo_ppotorch_score_500.pth"



def generate_data():
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}")
        print("è¯·æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä¸æˆªå›¾ä¸€è‡´ï¼")
        return

    # 1åˆå§‹åŒ–ç¯å¢ƒ
    env = gym.make(ENV_NAME)
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n

    # åŠ è½½ Agent
    print(f" æ­£åœ¨åŠ è½½ {ALGO.upper()} æ¨¡å‹: {MODEL_PATH} ...")
    
    if ALGO == "ppo":
        agent = SimplePPO(obs_dim, act_dim)
        # PPO åŠ è½½é€»è¾‘
        agent.load_model(MODEL_PATH) 
        
    elif ALGO == "doubledqn":
        agent = DoubleDQNAgent(obs_dim, act_dim)
        # DoubleDQN åŠ è½½é€»è¾‘
        agent.load(MODEL_PATH)
        
    else:
        raise ValueError("Unknown Algorithm")

    # å¼€å§‹é‡‡é›†æ•°æ®
    collected_states = []
    collected_actions = []
    
    obs, _ = env.reset(seed=2024)
    current_samples = 0
    
    print("å¼€å§‹é‡‡é›†ä¸“å®¶æ¼”ç¤ºæ•°æ®...")
    
    while current_samples < NUM_SAMPLES:
        # === æ ¸å¿ƒï¼šè·å–ç¡®å®šçš„ä¸“å®¶åŠ¨ä½œ (å»é™¤éšæœºæ€§) ===
        if ALGO == "doubledqn":
            # DoubleDQN æä¾›äº† evaluation_mode
            state_in = np.reshape(obs, (1, obs_dim))
            action = agent.act(state_in, evaluation_mode=True)
            
        elif ALGO == "ppo":
            # PPO éœ€è¦æ‰‹åŠ¨å–æœ€å¤§æ¦‚ç‡ (Argmax)ï¼Œç¡®ä¿æ˜¯ä¸“å®¶è¡Œä¸º
            state_t = torch.FloatTensor(obs).unsqueeze(0).to(agent.actor.net[0].weight.device)
            with torch.no_grad():
                probs = agent.actor(state_t).cpu().numpy()[0]
            action = np.argmax(probs) # å–æ¦‚ç‡æœ€å¤§çš„åŠ¨ä½œ

        # è®°å½•æ•°æ®
        collected_states.append(obs)
        collected_actions.append(action)
        
        # ç¯å¢ƒæ¨è¿›ä¸€æ­¥
        next_obs, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        if done:
            obs, _ = env.reset()
        else:
            obs = next_obs
            
        current_samples += 1
        if current_samples % 1000 == 0:
            print(f"   å·²æ”¶é›†: {current_samples}/{NUM_SAMPLES}")

    # ä¿å­˜ä¸º .npz æ ¼å¼
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    save_path = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)
    
    # æˆªå–åˆ°åˆšå¥½ NUM_SAMPLES ä¸ª
    final_obs = np.array(collected_states)[:NUM_SAMPLES]
    final_acts = np.array(collected_actions)[:NUM_SAMPLES]
    
    np.savez(save_path, obs=final_obs, actions=final_acts)
    
    print("\n" + "="*40)
    print(f" æˆåŠŸï¼ä¸“å®¶æ•°æ®å·²ä¿å­˜è‡³: {save_path}")
    print(f" æ•°æ®å½¢çŠ¶: States={final_obs.shape}, Actions={final_acts.shape}")
    print("="*40)
    print("ğŸ‘‰ ä¸‹ä¸€æ­¥: è¿è¡Œ 'python behavioral_cloning.py' (æˆ–ä½ çš„BCè®­ç»ƒè„šæœ¬) æ¥è®­ç»ƒæ¨¡ä»¿è€…ã€‚")

if __name__ == "__main__":
    generate_data()