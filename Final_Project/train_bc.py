import numpy as np
import gymnasium as gym
import os
import time
import matplotlib.pyplot as plt
import torch

# å¯¼å…¥ PPO (ä½œä¸ºä¸“å®¶ï¼Œç”¨äºç”Ÿæˆæ•°æ®)
from agents.simple_ppo_torch import SimplePPO
# å¯¼å…¥ BC (ä½œä¸ºå­¦ç”Ÿ)
from agents.behavioral_cloning import BCAgent, BCConfig

ENV_NAME = "CartPole-v1"

EXPERT_MODEL_PATH = r"Final_Project\models\best_ppo_ppotorch_score_500.pth"


DATA_NUM_SAMPLES = 5000  # è¶³å¤Ÿå¤šçš„æ ·æœ¬
BC_EPOCHS = 50          # è®­ç»ƒè½®æ•°
EVAL_EPISODES = 5       # æ¯ä¸ª Epoch è¯„ä¼°çš„å±€æ•°
PLOT_SAVE_PATH = "scores/bc_training_progress_graph.png"
MODEL_SAVE_PATH = "models/bc_trained_model.pth"
GOAL_SCORE = 475        # ç›®æ ‡åˆ†æ•°çº¿


# --- å¯è§†åŒ–å™¨ç±» ---
class TrainingVisualizer:
    def __init__(self, save_path):
        self.epochs = []
        self.scores = []
        self.save_path = save_path
        self.goal = GOAL_SCORE 

    def update(self, epoch, score):
        self.epochs.append(epoch)
        self.scores.append(score)

    def save_plot(self):
        if not self.scores:
            return

        plt.figure(figsize=(10, 6))
        
        # ç»˜åˆ¶åŸå§‹åˆ†æ•° 
        plt.plot(self.epochs, self.scores, label='Score per Epoch', color='#1f77b4', alpha=0.9)
        
        # è®¡ç®—å¹¶ç»˜åˆ¶æœ€è¿‘ 10 å±€å¹³å‡åˆ† (Average of last 10) 
      
        moving_avgs = []
        target_window = 10  
        window_size = min(target_window, len(self.scores)) # çª—å£å¤§å°ä¸è¶…è¿‡å½“å‰å·²æœ‰çš„åˆ†æ•°æ•°é‡
        
        for i in range(len(self.scores)):
            start_idx = max(0, i - window_size + 1)
            subset = self.scores[start_idx : i + 1]
            moving_avgs.append(np.mean(subset))
            
        plt.plot(self.epochs, moving_avgs, label=f'Average of last {target_window}', 
                 color='#ff7f0e', linestyle='--', linewidth=2)

        # 3. ç»˜åˆ¶ç›®æ ‡çº¿ (ç»¿è‰²ç‚¹çº¿)
        plt.axhline(y=self.goal, color='green', linestyle=':', label=f'Goal ({self.goal} Avg)', alpha=0.8)

        # 4. ç»˜åˆ¶è¶‹åŠ¿çº¿ (çº¢è‰²ç‚¹åˆ’çº¿)
        if len(self.epochs) > 1:
            z = np.polyfit(self.epochs, self.scores, 1)
            p = np.poly1d(z)
            plt.plot(self.epochs, p(self.epochs), "r-.", label='Trend', linewidth=1.5)

        plt.title(f"{ENV_NAME} - Behavioral Cloning Training Progress")
        plt.xlabel("Training Epochs")
        plt.ylabel("Evaluation Score")
        plt.legend(loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 520) 
        
        if not os.path.exists(os.path.dirname(self.save_path)):
            os.makedirs(os.path.dirname(self.save_path))
        plt.savefig(self.save_path)
        plt.close()

# --- æ•°æ®ç”Ÿæˆå‡½æ•° ---
def generate_expert_data(expert_agent, env, num_samples):
    states = []
    actions = []
    
    obs, _ = env.reset(seed=int(time.time()))
    while len(states) < num_samples:
        state_t = torch.FloatTensor(obs).unsqueeze(0).to(expert_agent.actor.net[0].weight.device)
        with torch.no_grad():
            probs = expert_agent.actor(state_t).cpu().numpy()[0]
        action = np.argmax(probs)
        
        states.append(obs)
        actions.append(action)
        
        obs, _, terminated, truncated, _ = env.step(action)
        if terminated or truncated:
            obs, _ = env.reset()
            
    return np.array(states), np.array(actions)

# --- è¯„ä¼°å‡½æ•° ---
def evaluate_bc_agent(student_agent, env, episodes=EVAL_EPISODES):
    scores = []
    for _ in range(episodes):
        state, _ = env.reset()
        steps = 0
        done = False
        while not done:
            action = student_agent.act(state, evaluation_mode=True)
            state, _, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            steps += 1
        scores.append(steps)
    return np.mean(scores)


def train_bc_with_visualization():
    # æ£€æŸ¥ä¸“å®¶æ¨¡å‹
    if not os.path.exists(EXPERT_MODEL_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°ä¸“å®¶æ¨¡å‹æ–‡ä»¶: {EXPERT_MODEL_PATH}")
        return

    # åˆå§‹åŒ–ç¯å¢ƒå’Œä¸“å®¶
    env = gym.make(ENV_NAME)
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n
    
    expert_agent = SimplePPO(obs_dim, act_dim)
    try:
        expert_agent.load_model(EXPERT_MODEL_PATH)
        print(f"ğŸš€ PPOä¸“å®¶æ¨¡å‹åŠ è½½æˆåŠŸï¼š{EXPERT_MODEL_PATH}")
    except Exception as e:
        print(f"âŒ åŠ è½½PPOä¸“å®¶æ¨¡å‹å¤±è´¥: {e}")
        return

    # ç”Ÿæˆé«˜è´¨é‡çš„ä¸“å®¶æ•°æ®é›†
    print(f"\nğŸ’¡ æ­£åœ¨ç”Ÿæˆé«˜è´¨é‡ä¸“å®¶æ•°æ®é›† (N={DATA_NUM_SAMPLES})...")
    states, actions = generate_expert_data(expert_agent, env, DATA_NUM_SAMPLES)
    
    # åˆå§‹åŒ– BC å­¦ç”Ÿ Agent
    print(f"\nâš™ï¸ åˆå§‹åŒ– BC å­¦ç”Ÿ Agent...")
    bc_cfg = BCConfig(epochs=BC_EPOCHS, batch_size=64, lr=0.001)
    student_agent = BCAgent(obs_dim, act_dim, cfg=bc_cfg)
    

    visualizer = TrainingVisualizer(save_path=PLOT_SAVE_PATH)

    #  BC è®­ç»ƒå¾ªç¯
    print(f"\n--- å¼€å§‹ BC è®­ç»ƒ (å…± {BC_EPOCHS} Epochs) ---")
    
   
    states_tensor = torch.FloatTensor(states).to(student_agent.device)
    actions_tensor = torch.LongTensor(actions).to(student_agent.device)
    dataset_size = len(states)
    indices = np.arange(dataset_size)
    
    student_agent.model.train() 

    for epoch in range(1, BC_EPOCHS + 1):
        # --- è®­ç»ƒä¸€ä¸ª Epoch ---
        np.random.shuffle(indices)
        epoch_loss = 0
        batches = 0
        
        for start_idx in range(0, dataset_size, bc_cfg.batch_size):
            end_idx = min(start_idx + bc_cfg.batch_size, dataset_size)
            batch_idx = indices[start_idx:end_idx]
            
            batch_s = states_tensor[batch_idx]
            batch_a = actions_tensor[batch_idx]
            
            student_agent.optimizer.zero_grad()
            logits = student_agent.model(batch_s)
            loss = student_agent.criterion(logits, batch_a)
            loss.backward()
            student_agent.optimizer.step()
            
            epoch_loss += loss.item()
            batches += 1
        
        avg_epoch_loss = epoch_loss / batches

       
        student_agent.model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        current_eval_score = evaluate_bc_agent(student_agent, env, episodes=EVAL_EPISODES)
        student_agent.model.train() # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
        
        print(f"Epoch {epoch}/{BC_EPOCHS} | Loss: {avg_epoch_loss:.4f} | Eval Score: {current_eval_score:.1f}")
        
        visualizer.update(epoch, current_eval_score)
        visualizer.save_plot() 

    
    student_agent.save(MODEL_SAVE_PATH)
    env.close()
    print(f"\nâœ… BC è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {MODEL_SAVE_PATH}")
    print(f"ğŸ“Š è®­ç»ƒè¿›åº¦å›¾å·²ä¿å­˜è‡³: {PLOT_SAVE_PATH}")

if __name__ == "__main__":
    os.makedirs("scores", exist_ok=True)
    os.makedirs("models", exist_ok=True)
    train_bc_with_visualization()