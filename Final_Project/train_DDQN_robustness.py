from __future__ import annotations
import numpy as np
import gymnasium as gym
import torch
import time
import os
import config 

from agents.double_dqn_torch import DoubleDQNAgent
from scores.score_logger import ScoreLogger

ENV_NAME = "CartPole-v1"
MODEL_DIR = "models"

# å†…éƒ¨æ•…éšœ
ACTION_NOISE_PROB = 0.0     # åŠ¨ä½œå™ªå£° (0.1, 0.15, 0.2)

# ç¯å¢ƒå™ªå£°-
STATE_NOISE_STD = 0.01       # çŠ¶æ€å™ªå£° (0.01, 0.05, 0.1)

# å¥–åŠ±ä¿¡å·å¹²æ‰° 
REWARD_NOISE_STD = 0.0      # å¥–åŠ±å™ªå£° (1.0, 2.0)
SPARSE_REWARD = False       # ç¨€ç–å¥–åŠ± (True)

# å¤–éƒ¨æ”»å‡»
ADVERSARIAL_ATTACK = False  # Trueå¼€å¯
ATTACK_INTERVAL = 150       # æ¯éš”å¤šå°‘æ­¥æ”»å‡»ä¸€æ¬¡
ATTACK_FORCE = 0.5          # æ”»å‡»å¼ºåº¦ (0.5, 1.0, 2.0)

def train_ddqn_robust():
    suffix = ""
    if ACTION_NOISE_PROB > 0: suffix += f"_ActionNoise_{ACTION_NOISE_PROB}"
    if STATE_NOISE_STD > 0: suffix += f"_StateNoise_{STATE_NOISE_STD}" # ğŸ”¥ æ–°å¢åç¼€
    if REWARD_NOISE_STD > 0: suffix += f"_RewardNoise_{REWARD_NOISE_STD}"
    if SPARSE_REWARD: suffix += "_Sparse"
    if ADVERSARIAL_ATTACK: suffix += f"_AdvAttack_F{ATTACK_FORCE}_I{ATTACK_INTERVAL}"
    
    if not suffix: suffix = "_Baseline"
        
    logger_name = f"{ENV_NAME}_DDQN{suffix}"
    score_logger = ScoreLogger(logger_name)
    
    print(f" å¯åŠ¨ Double DQN é²æ£’æ€§è®­ç»ƒ: {logger_name} ")
    print(f"å¹²æ‰°è®¾ç½®: åŠ¨ä½œ={ACTION_NOISE_PROB}, çŠ¶æ€={STATE_NOISE_STD}, å¥–åŠ±={REWARD_NOISE_STD}, ç¨€ç–={SPARSE_REWARD}, å¯¹æŠ—={ADVERSARIAL_ATTACK}")

    env = gym.make(ENV_NAME)
    obs_dim = env.observation_space.shape[0]
    act_dim = env.action_space.n
    
    # åˆå§‹åŒ– DDQN Agent
    agent = DoubleDQNAgent(obs_dim, act_dim)
    
    print(f"[Info] Using device: {agent.device}")

    checkpoint_dir = "checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)
    best_score = 0

    for episode in range(1, config.TOTAL_EPISODES + 1):
        state, _ = env.reset(seed=episode)
        
        # åˆå§‹çŠ¶æ€å™ªå£°
        if STATE_NOISE_STD > 0:
            state += np.random.normal(0, STATE_NOISE_STD, state.shape)
            
        state = np.reshape(state, (1, obs_dim))
        
        steps = 0
        done = False
        
        while not done:
            steps += 1
            
            # Agenté€‰æ‹©åŠ¨ä½œ
            intended_action = agent.act(state)
            
            # åŠ¨ä½œå¹²æ‰°æ”»å‡»
            real_action = intended_action
            if np.random.rand() < ACTION_NOISE_PROB:
                real_action = env.action_space.sample() 
            
            # ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œ
            next_state_raw, reward, terminated, truncated, _ = env.step(real_action)
            
            # å¤–éƒ¨å†²å‡»
            if ADVERSARIAL_ATTACK and steps > 0 and steps % ATTACK_INTERVAL == 0:
                print(f"   Ep:{episode}, Step:{steps} -> DDQN é­å—æ”»å‡»ï¼")
                current_env_state = list(env.unwrapped.state)
                
                direction = 1 if np.random.rand() > 0.5 else -1
                current_env_state[3] += direction * ATTACK_FORCE
                
                env.unwrapped.state = tuple(current_env_state)
                # æ›´æ–°ç‰©ç†çŠ¶æ€
                next_state_raw = np.array(current_env_state, dtype=np.float32)
           
            # è§‚æµ‹å™ªå£°
            if STATE_NOISE_STD > 0:
                # ç»™è§‚æµ‹å€¼åŠ å™ªå£°
                noise = np.random.normal(0, STATE_NOISE_STD, next_state_raw.shape)
                next_state_raw += noise

            # çŠ¶æ€å¤„ç†
            done = terminated or truncated
            next_state = np.reshape(next_state_raw, (1, obs_dim))

            # å¥–åŠ±é€»è¾‘
            if SPARSE_REWARD:
                # ç¨€ç–å¥–åŠ±
                if not done:
                    reward = 0
                else:
                    reward = 100 if steps >= 490 else -1
            else:
                # æ­£å¸¸æ¨¡å¼
                # åŸºç¡€æƒ©ç½š
                if done and steps < 500:
                    reward = -1.0
                # å¥–åŠ±å™ªå£°
                if REWARD_NOISE_STD > 0:
                    reward += np.random.normal(0, REWARD_NOISE_STD)

            # Agentå­¦ä¹ 
            agent.step(state, real_action, reward, next_state, done)
            
            state = next_state

        epsilon = getattr(agent, 'exploration_rate', 0.0)
        score_logger.add_score(steps, episode)
        
        if steps > best_score: 
            best_score = steps
            agent.save(f"{checkpoint_dir}/best_ddqn_robust_model.torch")

        if episode % 10 == 0:
            print(f"Ep: {episode}, Epsilon: {epsilon:.3f}, Score: {steps}, Best: {best_score}")

    env.close()
    return agent

if __name__ == "__main__":
    train_ddqn_robust()
